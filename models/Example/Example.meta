vocab_size: 939
embed_dim: 512
n_heads: 8
ff_dim: 1024
n_layers: 4
dropout: 0.3
lr: 0.00025
epochs: 50
weight_decay: 0.0001
batch_size: 64
train_size: 50000
val_size: 10000
test_size: 10000
vocab_path: C:\Users\lukew\Desktop\slm\data\tokens.txt
max_seq_len: 2048
model_name: Example
saved_date: 2025-09-22
saved_time: 13:18
